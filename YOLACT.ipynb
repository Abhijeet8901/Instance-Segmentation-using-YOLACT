{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOLACT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPYT7kHiNGiwTH1nADWUeQ9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhijeet8901/Instance-Segmentation-using-YOLACT/blob/main/YOLACT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5sY7gsWv39A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db21efbe-dc69-4321-8a4e-d4fb9073a41e"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.nn import init\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision\r\n",
        "import torchvision.transforms as T\r\n",
        "import torch.optim as optim\r\n",
        "import torch.utils.data as data\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\r\n",
        "from torch.utils.data.distributed import DistributedSampler\r\n",
        "from torch.utils.data import sampler\r\n",
        "import torchvision.datasets as dset\r\n",
        "\r\n",
        "import time\r\n",
        "from google.colab.patches import cv2_imshow\r\n",
        "import pickle\r\n",
        "import cv2\r\n",
        "import numpy as np\r\n",
        "import glob\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.gridspec as gridspec\r\n",
        "from torchsummary import summary\r\n",
        "\r\n",
        "dtype = torch.cuda.FloatTensor\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "print(device)\r\n",
        "\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "import sys\r\n",
        "sys.path.append('drive/My Drive/YOLACT')\r\n",
        "from box_utils import *\r\n",
        "from coco import *\r\n",
        "from augmentations import *\r\n",
        "from config import *"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkgErbaBfaF8"
      },
      "source": [
        "class set_config:\r\n",
        "\r\n",
        "  def __init__(self):\r\n",
        "    self.img_size=550\r\n",
        "    self.num_anchors=3\r\n",
        "    self.num_classes=21\r\n",
        "    self.mode='train'\r\n",
        "    self.scales=[32, 64, 128, 256, 512]\r\n",
        "    self.aspect_ratios  = [1, 1 / 2, 2]\r\n",
        "    self.batch_size=8\r\n",
        "    self.pos_iou_thre=0.5\r\n",
        "    self.neg_iou_thre=0.4\r\n",
        "    self.np_ratio=3\r\n",
        "    self.conf_alpha = 1\r\n",
        "    self.bbox_alpha = 1.5\r\n",
        "    self.mask_alpha = 6.125\r\n",
        "    self.semantic_alpha = 1\r\n",
        "    self.max_detections = 100\r\n",
        "    self.resume=True\r\n",
        "    self.test_image_path=\"drive/My Drive/Personal/Abhijeet.jpg\"\r\n",
        "    self.video=None#\"drive/My Drive/Personal/test_video.mp4\"\r\n",
        "    self.label_id={}\r\n",
        "    for idx,class1 in enumerate(PASCAL_CLASSES):\r\n",
        "      self.label_id[class1]=idx\r\n",
        "    self.lr=0.001\r\n",
        "    self.cfg_name=\"resnet_pascal\"\r\n",
        "    self.save_lincomb=False\r\n",
        "    self.visual_thre=0.3\r\n",
        "    self.nms_score_thre = 0.05\r\n",
        "    self.nms_iou_thre = 0.5\r\n",
        "    self.top_k = 200\r\n",
        "    self.no_crop=False\r\n",
        "    self.hide_mask=False\r\n",
        "    self.hide_bbox=False\r\n",
        "    self.hide_score=False\r\n",
        "    self.cutout=False\r\n",
        "    self.real_time=False\r\n",
        "    self.class_names = PASCAL_CLASSES\r\n",
        "\r\n",
        "cfg=set_config()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wp7r-MEifUVw"
      },
      "source": [
        "class BottleNeck(nn.Module):\r\n",
        "  \r\n",
        "    def __init__(self,inplanes,planes,stride=1,downsample=None):\r\n",
        "        super().__init__()\r\n",
        "        self.conv1=nn.Conv2d(inplanes,planes,kernel_size=1,bias=False)\r\n",
        "        self.bn1=nn.BatchNorm2d(planes)\r\n",
        "        self.conv2=nn.Conv2d(planes,planes,kernel_size=3,stride=stride,padding=1,bias=False)\r\n",
        "        self.bn2=nn.BatchNorm2d(planes)\r\n",
        "        self.conv3=nn.Conv2d(planes,4*planes,kernel_size=1,bias=False)\r\n",
        "        self.bn3=nn.BatchNorm2d(4*planes)\r\n",
        "        self.relu=nn.ReLU(inplace=True)\r\n",
        "        self.downsample=downsample\r\n",
        "\r\n",
        "    def forward(self,x):\r\n",
        "\r\n",
        "        residual=x\r\n",
        "        out=self.conv1(x)\r\n",
        "        out=self.bn1(out)\r\n",
        "        out=self.relu(out)\r\n",
        "        out=self.conv2(out)\r\n",
        "        out=self.bn2(out)\r\n",
        "        out=self.relu(out)\r\n",
        "        out=self.conv3(out)\r\n",
        "        out=self.bn3(out)\r\n",
        "        \r\n",
        "        if self.downsample != None:\r\n",
        "          residual=self.downsample(x)\r\n",
        "        \r\n",
        "        out+=residual\r\n",
        "        out=self.relu(out)\r\n",
        "\r\n",
        "        return out\r\n",
        "\r\n",
        "class Resnet_Backbone(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.inplanes=64\r\n",
        "\r\n",
        "        self.conv1=nn.Conv2d(3,64,kernel_size=7,stride=2,padding=3,bias=False)\r\n",
        "        self.bn1=nn.BatchNorm2d(64)\r\n",
        "        self.relu=nn.ReLU(inplace=True)\r\n",
        "        self.pool=nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\r\n",
        "\r\n",
        "        self.layer1=self.set_bottleneck(64,3)\r\n",
        "        self.layer2=self.set_bottleneck(128,4,stride=2)\r\n",
        "        self.layer3=self.set_bottleneck(256,6,stride=2)\r\n",
        "        self.layer4=self.set_bottleneck(512,3,stride=2)\r\n",
        "\r\n",
        "        self.backbone_modules = [m for m in self.modules() if isinstance(m, nn.Conv2d)]\r\n",
        "\r\n",
        "    def set_bottleneck(self,planes,num_of_necks,stride=1):\r\n",
        "\r\n",
        "        downsample=None\r\n",
        "        layers=[]\r\n",
        "        if stride !=1 or self.inplanes != 4*planes:\r\n",
        "          downsample=nn.Sequential(nn.Conv2d(self.inplanes,4*planes,kernel_size=1,\r\n",
        "                                              stride=stride,bias=False),\r\n",
        "                                    nn.BatchNorm2d(4*planes))\r\n",
        "        layers.append(BottleNeck(self.inplanes,planes,stride,downsample))\r\n",
        "        \r\n",
        "        num_of_necks-=1\r\n",
        "        self.inplanes=4*planes\r\n",
        "        \r\n",
        "        for i in range(num_of_necks):\r\n",
        "          layers.append(BottleNeck(self.inplanes,planes))\r\n",
        "        \r\n",
        "        return nn.Sequential(*layers)\r\n",
        "  \r\n",
        "    def forward(self,x):\r\n",
        "\r\n",
        "        out=self.pool(self.relu(self.bn1(self.conv1(x))))\r\n",
        "\r\n",
        "        outs=[]\r\n",
        "\r\n",
        "        out=self.layer1(out)\r\n",
        "        out=self.layer2(out)\r\n",
        "        outs.append(out)\r\n",
        "        out=self.layer3(out)\r\n",
        "        outs.append(out)\r\n",
        "        out=self.layer4(out)\r\n",
        "        outs.append(out)\r\n",
        "\r\n",
        "        return tuple(outs)\r\n",
        "    \r\n",
        "    def init_backbone(self,state_dic):\r\n",
        "      self.load_state_dict(state_dic,strict=False)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJWeps_oiDCh"
      },
      "source": [
        "class FPN(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.sizes=[512,1024,2048]\r\n",
        "        self.lat_layers=nn.ModuleList()\r\n",
        "        self.dealiasing_layers=nn.ModuleList()\r\n",
        "        self.downsample_layer=nn.ModuleList()\r\n",
        "        for i in range(3):\r\n",
        "          self.lat_layers.append(nn.Conv2d(self.sizes[i],256,kernel_size=1))\r\n",
        "          self.dealiasing_layers.append(nn.Conv2d(256,256,kernel_size=3,padding=1))\r\n",
        "          if i!=2:\r\n",
        "            self.downsample_layer.append(nn.Conv2d(256,256,kernel_size=3,stride=2,padding=1))\r\n",
        "\r\n",
        "    def forward(self,input_outs):\r\n",
        "\r\n",
        "        outs=[]\r\n",
        "        x=x = torch.zeros(1,device=device)\r\n",
        "        for i in reversed(range(3)):\r\n",
        "          if i!=2:\r\n",
        "            _,_,h,w=input_outs[i].shape\r\n",
        "            x= F.interpolate(x,size=(h,w),mode='bilinear',align_corners=False)\r\n",
        "\r\n",
        "          x=x+ self.lat_layers[i](input_outs[i])\r\n",
        "\r\n",
        "          outs.append(F.relu(self.dealiasing_layers[i](x)))\r\n",
        "        \r\n",
        "        outs.reverse()\r\n",
        "\r\n",
        "        for i in range(2):\r\n",
        "          outs.append(self.downsample_layer[i](outs[-1]))\r\n",
        "\r\n",
        "        return outs\r\n",
        "   "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gArIM8pO7z2G"
      },
      "source": [
        "class ProtoNet(nn.Module):\r\n",
        "\r\n",
        "  def __init__(self):\r\n",
        "      super().__init__()\r\n",
        "\r\n",
        "      self.layer1=nn.ModuleList()\r\n",
        "      for i in range(3):\r\n",
        "        self.layer1.append(nn.Conv2d(256,256,kernel_size=3,padding=1))\r\n",
        "        self.layer1.append(nn.ReLU(inplace=True))\r\n",
        "      self.layer2=nn.Conv2d(256,256,kernel_size=3,padding=1)\r\n",
        "      self.relu=nn.ReLU(inplace=True)\r\n",
        "      self.layer3=nn.Conv2d(256,32,kernel_size=1)\r\n",
        "  \r\n",
        "  def forward(self,x):\r\n",
        "\r\n",
        "      out=x\r\n",
        "      for layer in self.layer1:\r\n",
        "        out=layer(out)\r\n",
        "      out=F.interpolate(out,(138,138),mode='bilinear',align_corners=False)\r\n",
        "      out=self.relu(out)\r\n",
        "      out=self.layer2(out)\r\n",
        "      out=self.relu(out)\r\n",
        "      out=self.layer3(out)\r\n",
        "      out=self.relu(out)\r\n",
        "\r\n",
        "      return out"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHXKEnKfd9sa"
      },
      "source": [
        "class PredictionNet(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "      super().__init__()\r\n",
        "\r\n",
        "      self.layer1=nn.Conv2d(256,256,kernel_size=3,padding=1)\r\n",
        "      self.relu=nn.ReLU(inplace=True)\r\n",
        "      self.box_layer=nn.Conv2d(256,cfg.num_anchors*4,kernel_size=3,padding=1)\r\n",
        "      self.class_layer=nn.Conv2d(256,cfg.num_anchors*cfg.num_classes,kernel_size=3,padding=1)\r\n",
        "      self.mask_layer=nn.Conv2d(256,cfg.num_anchors*32,kernel_size=3,padding=1)\r\n",
        "\r\n",
        "    def forward(self,x):\r\n",
        "\r\n",
        "      out=self.layer1(x)\r\n",
        "      out=self.relu(out)\r\n",
        "      box_pred=self.box_layer(out).permute(0,2,3,1).reshape(out.size(0),-1,4)\r\n",
        "      class_pred=self.class_layer(out).permute(0,2,3,1).reshape(out.size(0),-1,cfg.num_classes)\r\n",
        "      mask_pred=self.mask_layer(out).permute(0,2,3,1).reshape(out.size(0),-1,32)\r\n",
        "\r\n",
        "      return class_pred,box_pred,mask_pred"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBVF4i9pkusW"
      },
      "source": [
        "class YOLACT(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.backbone=Resnet_Backbone()\r\n",
        "        self.fpn=FPN()\r\n",
        "        self.protonet=ProtoNet()\r\n",
        "        self.prediction_layers=PredictionNet()\r\n",
        "\r\n",
        "        if cfg.mode=='train':\r\n",
        "          self.semantic_layer=nn.Conv2d(256,cfg.num_classes-1,kernel_size=1)\r\n",
        "    \r\n",
        "    def forward(self,img,box_class_gt=None,mask_gt=None):\r\n",
        "\r\n",
        "        backbone_outs=self.backbone(img)\r\n",
        "        fpn_outs=self.fpn(backbone_outs)\r\n",
        "        proto_out=self.protonet(fpn_outs[0])\r\n",
        "        proto_out=proto_out.permute(0,2,3,1).contiguous()\r\n",
        "\r\n",
        "        if cfg.mode == 'train':\r\n",
        "          seg_out=self.semantic_layer(fpn_outs[0])\r\n",
        "\r\n",
        "        class_pred,box_pred,mask_pred=[],[],[]\r\n",
        "\r\n",
        "        for fpn_out in fpn_outs:\r\n",
        "          class_p,box_p,mask_p=self.prediction_layers(fpn_out)\r\n",
        "          class_pred.append(class_p)\r\n",
        "          box_pred.append(box_p)\r\n",
        "          mask_pred.append(mask_p)\r\n",
        "\r\n",
        "        class_pred=torch.cat(class_pred,dim=1)\r\n",
        "        box_pred=torch.cat(box_pred,dim=1)\r\n",
        "        mask_pred=torch.cat(mask_pred,dim=1)\r\n",
        "\r\n",
        "        anchors,shapes=[],[69,35,18,9,5]\r\n",
        "\r\n",
        "        for i in range(5):\r\n",
        "          anchors+=make_anchors(cfg,shapes[i],shapes[i],cfg.scales[i])\r\n",
        "\r\n",
        "        anchors = torch.tensor(anchors, device=device).reshape(-1, 4)\r\n",
        "        \r\n",
        "        if self.training:\r\n",
        "          return self.compute_loss(class_pred,box_pred,mask_pred,proto_out,seg_out,box_class_gt,mask_gt,anchors)\r\n",
        "        \r\n",
        "        else :\r\n",
        "          class_pred=F.softmax(class_pred,-1)\r\n",
        "          return class_pred,box_pred,mask_pred,proto_out,anchors\r\n",
        "\r\n",
        "    def init_weights(self,state_dic):\r\n",
        "\r\n",
        "        self.backbone.init_backbone(state_dic)\r\n",
        "\r\n",
        "        for name,module in self.named_modules():\r\n",
        "\r\n",
        "          if isinstance(module,nn.Conv2d) and module not in self.backbone.backbone_modules:\r\n",
        "            nn.init.xavier_uniform_(module.weight.data)\r\n",
        "\r\n",
        "            if module.bias is not None:\r\n",
        "              module.bias.data.zero_()\r\n",
        "    def load_weights(self,path):\r\n",
        "\r\n",
        "        if torch.cuda.is_available():\r\n",
        "            state_dict = torch.load(path)\r\n",
        "        else:\r\n",
        "            state_dict = torch.load(path, map_location='cpu')\r\n",
        "        for key in list(state_dict.keys()):\r\n",
        "          \r\n",
        "          if key.startswith('fpn.downsample'):\r\n",
        "            if int(key.split('.')[2])>=2:\r\n",
        "              del state_dict[key]\r\n",
        "          \r\n",
        "          if cfg.mode!= 'train' and key.startswith('semantic'):\r\n",
        "            del state_dict[key]\r\n",
        "        \r\n",
        "        self.load_state_dict(state_dict)\r\n",
        "\r\n",
        "    def compute_loss(self,class_p,box_p,coef_p,proto_p,seg_p,box_class_gt,mask_gt,anchors):\r\n",
        "        \r\n",
        "        class_gt=[None]*len(box_class_gt)\r\n",
        "        num_anchors=anchors.size(0)\r\n",
        "        batch_size=len(box_class_gt)\r\n",
        "        offsets=torch.zeros((batch_size,num_anchors,4),dtype=torch.float32,device=device)\r\n",
        "        conf_gt = torch.zeros((batch_size, num_anchors), dtype=torch.int64,device=device)\r\n",
        "        anchor_max_gt = torch.zeros((batch_size, num_anchors, 4), dtype=torch.float32,device=device)\r\n",
        "        anchor_max_i = torch.zeros((batch_size, num_anchors), dtype=torch.int64,device=device)\r\n",
        "\r\n",
        "        for i in range(len(box_class_gt)):\r\n",
        "          class_gt[i]=box_class_gt[i][:,-1].long()\r\n",
        "\r\n",
        "          offsets[i],conf_gt[i],anchor_max_gt[i],anchor_max_i[i]=match(cfg,box_class_gt[i][:,:-1],anchors,class_gt[i])\r\n",
        "\r\n",
        "        pos_bool = conf_gt>0\r\n",
        "\r\n",
        "        loss_c = self.category_loss(class_p,conf_gt,pos_bool)\r\n",
        "        loss_b = self.box_loss(box_p, offsets, pos_bool)\r\n",
        "        loss_m = self.lincomb_mask_loss(pos_bool, anchor_max_i, coef_p, proto_p, mask_gt, anchor_max_gt)\r\n",
        "        loss_s = self.semantic_seg_loss(seg_p, mask_gt, class_gt)\r\n",
        "\r\n",
        "        return loss_c,loss_b,loss_m,loss_s\r\n",
        "\r\n",
        "    def category_loss(self,class_p,conf_gt,pos_bool):\r\n",
        "\r\n",
        "        # Negative examples may be way more than positive examples, so we dont use all negative examples, hence this bullshit\r\n",
        "        batch_conf=class_p.reshape(-1,cfg.num_classes)\r\n",
        "\r\n",
        "        max_conf=batch_conf.max()\r\n",
        "        mark=torch.log(torch.sum(torch.exp(batch_conf-max_conf),1)) + max_conf - batch_conf[:,0]\r\n",
        "\r\n",
        "        mark=mark.reshape(class_p.size(0),-1)\r\n",
        "        mark[pos_bool]=0\r\n",
        "        mark[conf_gt<0]=0\r\n",
        "\r\n",
        "        _, idx = mark.sort(1, descending=True)\r\n",
        "        _, idx_rank = idx.sort(1)\r\n",
        "\r\n",
        "        num_pos = pos_bool.long().sum(1, keepdim=True)\r\n",
        "        num_neg = torch.clamp(cfg.np_ratio * num_pos, max=pos_bool.size(1) - 1)\r\n",
        "        neg_bool = idx_rank < num_neg.expand_as(idx_rank)\r\n",
        "\r\n",
        "        neg_bool[pos_bool] = 0\r\n",
        "        neg_bool[conf_gt < 0] = 0\r\n",
        "        class_p_mined = class_p[(pos_bool + neg_bool)].reshape(-1, cfg.num_classes)\r\n",
        "        class_gt_mined = conf_gt[(pos_bool + neg_bool)]\r\n",
        "\r\n",
        "        return cfg.conf_alpha * F.cross_entropy(class_p_mined, class_gt_mined, reduction='sum') / num_pos.sum()\r\n",
        "\r\n",
        "    def box_loss(self,box_p,offsets,pos_bool):\r\n",
        "\r\n",
        "        num_pos = pos_bool.sum()\r\n",
        "        pos_box_p = box_p[pos_bool, :]\r\n",
        "        pos_offsets = offsets[pos_bool, :]\r\n",
        "\r\n",
        "        return cfg.bbox_alpha * F.smooth_l1_loss(pos_box_p, pos_offsets, reduction='sum') / num_pos\r\n",
        "\r\n",
        "    def lincomb_mask_loss(self, pos_bool, anchor_max_i, coef_p, proto_p, mask_gt, anchor_max_gt):\r\n",
        "\r\n",
        "        proto_h, proto_w = proto_p.shape[1:3]\r\n",
        "        total_pos_num = pos_bool.sum()\r\n",
        "        loss_m = 0\r\n",
        "        for i in range(coef_p.size(0)):\r\n",
        "            downsampled_masks = F.interpolate(mask_gt[i].unsqueeze(0), (proto_h, proto_w), mode='bilinear',\r\n",
        "                                              align_corners=False).squeeze(0)\r\n",
        "\r\n",
        "            downsampled_masks = downsampled_masks.permute(1, 2, 0).contiguous()  \r\n",
        "            \r\n",
        "            downsampled_masks = downsampled_masks.gt(0.5).float()\r\n",
        "\r\n",
        "            pos_anchor_i = anchor_max_i[i][pos_bool[i]]\r\n",
        "            pos_anchor_box = anchor_max_gt[i][pos_bool[i]]\r\n",
        "            pos_coef = coef_p[i][pos_bool[i]]\r\n",
        "\r\n",
        "            if pos_anchor_i.size(0) == 0:\r\n",
        "                continue\r\n",
        "\r\n",
        "            old_num_pos = pos_coef.size(0)\r\n",
        "            if old_num_pos > cfg.max_detections:\r\n",
        "                perm = torch.randperm(pos_coef.size(0))\r\n",
        "                select = perm[:\r\n",
        "                              cfg.max_detections]\r\n",
        "                pos_coef = pos_coef[select]\r\n",
        "                pos_anchor_i = pos_anchor_i[select]\r\n",
        "                pos_anchor_box = pos_anchor_box[select]\r\n",
        "\r\n",
        "            num_pos = pos_coef.size(0)\r\n",
        "\r\n",
        "            pos_mask_gt = downsampled_masks[:, :, pos_anchor_i]\r\n",
        "\r\n",
        "            mask_p = torch.sigmoid(proto_p[i] @ pos_coef.t())\r\n",
        "            mask_p = crop_(mask_p, pos_anchor_box)\r\n",
        "            mask_loss = F.binary_cross_entropy(torch.clamp(mask_p, 0, 1), pos_mask_gt, reduction='none')\r\n",
        "\r\n",
        "            anchor_area = (pos_anchor_box[:, 2] - pos_anchor_box[:, 0]) * (pos_anchor_box[:, 3] - pos_anchor_box[:, 1])\r\n",
        "            mask_loss = mask_loss.sum(dim=(0, 1)) / anchor_area\r\n",
        "\r\n",
        "            if old_num_pos > num_pos:\r\n",
        "                mask_loss *= old_num_pos / num_pos\r\n",
        "\r\n",
        "            loss_m += torch.sum(mask_loss)\r\n",
        "\r\n",
        "        return cfg.mask_alpha * loss_m / proto_h / proto_w / total_pos_num\r\n",
        "\r\n",
        "    def semantic_seg_loss(self, segmentation_p, mask_gt, class_gt):\r\n",
        "\r\n",
        "        batch_size, num_classes, mask_h, mask_w = segmentation_p.size()\r\n",
        "        loss_s = 0\r\n",
        "\r\n",
        "        for i in range(batch_size):\r\n",
        "            cur_segment = segmentation_p[i]\r\n",
        "            cur_class_gt = class_gt[i]\r\n",
        "\r\n",
        "            downsampled_masks = F.interpolate(mask_gt[i].unsqueeze(0), (mask_h, mask_w), mode='bilinear',\r\n",
        "                                              align_corners=False).squeeze(0)\r\n",
        "            downsampled_masks = downsampled_masks.gt(0.5).float()\r\n",
        "\r\n",
        "            segment_gt = torch.zeros_like(cur_segment, requires_grad=False)\r\n",
        "            for j in range(downsampled_masks.size(0)):\r\n",
        "                segment_gt[cur_class_gt[j]] = torch.max(segment_gt[cur_class_gt[j]], downsampled_masks[j])\r\n",
        "\r\n",
        "            loss_s += F.binary_cross_entropy_with_logits(cur_segment, segment_gt, reduction='sum')\r\n",
        "\r\n",
        "        return cfg.semantic_alpha * loss_s / mask_h / mask_w / batch_size"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNkq264Qc_8v"
      },
      "source": [
        "# load weights for resnet to initialize with\r\n",
        "from torchvision.models.resnet import resnet50\r\n",
        "\r\n",
        "model=resnet50(pretrained=True)\r\n",
        "model=model.to(device)\r\n",
        "\r\n",
        "state_dict_pretrained=model.state_dict()\r\n",
        "keys=list(state_dict_pretrained)\r\n",
        "for i,key in enumerate(keys):\r\n",
        "  if i==318 or i==319:\r\n",
        "    state_dict_pretrained.pop(key)\r\n",
        "keys=list(state_dict_pretrained)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49-7Kz0SG-yu"
      },
      "source": [
        "from zipfile import ZipFile\r\n",
        "import zipfile\r\n",
        "filename='SegmentationObject.zip'\r\n",
        "with ZipFile(filename,'r') as zip:\r\n",
        "  zip.extractall()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDMUoY5R7cMI"
      },
      "source": [
        "from numpy import asarray\r\n",
        "def calc_mask(mask_path,num_objects):\r\n",
        "    img = Image.open(mask_path)\r\n",
        "    img=asarray(img)\r\n",
        "    mask=np.zeros((img.shape[0],img.shape[1],num_objects))\r\n",
        "    for i in range(num_objects):\r\n",
        "        mask[:,:,i]=(img==i+1)\r\n",
        "    mask=mask.transpose(2,0,1)\r\n",
        "    return mask"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0d_RzBrVVyE"
      },
      "source": [
        "class PascalLoader(data.Dataset): \r\n",
        "\r\n",
        "    def __init__(self,mode='train'):\r\n",
        "\r\n",
        "      self.mode=mode\r\n",
        "      self.cfg=cfg\r\n",
        "      if self.mode=='train' or self.mode=='val':\r\n",
        "        img_path=\"drive/My Drive/YOLACT/VOCdevkit/VOC2012/JPEGImages/*.*\"\r\n",
        "        mask_path=\"SegmentationObject/*.*\"\r\n",
        "        annot_file=\"drive/My Drive/YOLACT/annot_file.pkl\"\r\n",
        "        open_file=open(annot_file,'rb')\r\n",
        "        self.boxes=pickle.load(open_file)\r\n",
        "        open_file.close()\r\n",
        "        self.image_pathlist=glob.glob(img_path)\r\n",
        "        self.mask_pathlist=glob.glob(mask_path)\r\n",
        "        self.image_pathlist.sort()\r\n",
        "        self.mask_pathlist.sort()\r\n",
        "        self.boxes\r\n",
        "      \r\n",
        "      elif self.mode=='detect':\r\n",
        "        self.img_path=cfg.test_image_path\r\n",
        "\r\n",
        "    def __getitem__(self,index):\r\n",
        "\r\n",
        "      if self.mode=='detect':\r\n",
        "        image=cv2.imread(self.img_path)\r\n",
        "        image_normed=val_aug(image,cfg.img_size)\r\n",
        "        return image_normed,image,self.img_path.split('/')[-1]\r\n",
        "\r\n",
        "      else:\r\n",
        "        image_path=self.image_pathlist[index]\r\n",
        "        mask_path=self.mask_pathlist[index]\r\n",
        "        all_box=self.boxes[index]\r\n",
        "        img=cv2.imread(image_path)\r\n",
        "        mask=cv2.imread(mask_path)\r\n",
        "        height,width,_=img.shape\r\n",
        "        box_list, label_list = [], []\r\n",
        "        for box in all_box:\r\n",
        "          x1y1x2y2_box = np.array([box[0], box[1],box[2], box[3]]).astype(float)\r\n",
        "          category=self.cfg.label_id[box[4]]\r\n",
        "          box_list.append(x1y1x2y2_box)\r\n",
        "          label_list.append(category)\r\n",
        "        \r\n",
        "        if len(box_list)>0:\r\n",
        "\r\n",
        "          masks=calc_mask(mask_path,len(box_list))\r\n",
        "          boxes=np.array(box_list)\r\n",
        "          labels=np.array(label_list)\r\n",
        "\r\n",
        "          assert masks.shape==(boxes.shape[0], height, width)\r\n",
        "\r\n",
        "          if self.mode == 'train':\r\n",
        "            img, masks, boxes, labels = train_aug(img, masks, boxes, labels, self.cfg.img_size)\r\n",
        "            if img is None:\r\n",
        "              return None, None, None\r\n",
        "            else:\r\n",
        "              boxes = np.hstack((boxes, np.expand_dims(labels, axis=1)))\r\n",
        "              return img, boxes, masks\r\n",
        "          elif self.mode=='val':\r\n",
        "            img = val_aug(img, self.cfg.img_size)\r\n",
        "            boxes = boxes / np.array([width, height, width, height]) \r\n",
        "            boxes = np.hstack((boxes, np.expand_dims(labels, axis=1)))\r\n",
        "            return img, boxes, masks, height, width\r\n",
        "        else:\r\n",
        "          print(\"No valid image\")\r\n",
        "          return None,None,None\r\n",
        "      \r\n",
        "    def __len__(self):\r\n",
        "      if self.mode=='train':\r\n",
        "        return len(self.image_pathlist)\r\n",
        "      else:\r\n",
        "        return 1"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sNld0_YSyqV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74f65524-47fd-4f0e-af39-9d141f11ff74"
      },
      "source": [
        "######################### Let's Train this Bitch #############################\r\n",
        "import torch.backends.cudnn as cudnn\r\n",
        "import torch.distributed as dist\r\n",
        "torch.autograd.set_detect_anomaly(True)\r\n",
        "\r\n",
        "\r\n",
        "net=YOLACT().to(device)\r\n",
        "net.train()\r\n",
        "cfg.mode='train'\r\n",
        "if cfg.resume:\r\n",
        "  path=\"drive/My Drive/YOLACT/weights/latest_resnet_pascal_23.pth\"\r\n",
        "  if torch.cuda.is_available():\r\n",
        "    net.load_weights(path)\r\n",
        "  else:\r\n",
        "    net.load_weights(path)\r\n",
        "  start_step = int(path.split('.pth')[0].split('_')[-1])\r\n",
        "  print(f'\\nResume training with \\'{path}\\'.\\n')\r\n",
        "\r\n",
        "else:\r\n",
        "  net.init_weights(state_dict_pretrained)\r\n",
        "  start_step=0\r\n",
        "\r\n",
        "dataset = PascalLoader(mode='train')\r\n",
        "optimizer =optim.SGD(net.parameters(),lr=cfg.lr,momentum=0.9,weight_decay=5e-4)\r\n",
        "train_loader= DataLoader(dataset,cfg.batch_size,shuffle=True,collate_fn=train_collate)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Resume training with 'drive/My Drive/YOLACT/weights/latest_resnet_pascal_23.pth'.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkiY0dxv68HJ"
      },
      "source": [
        "################# Run this cell only for training, not for detection ############################\r\n",
        "training=True\r\n",
        "time_last=time.time()\r\n",
        "step_no=start_step+1\r\n",
        "while training:\r\n",
        "\r\n",
        "  for i,(images,boxes,masks) in enumerate(train_loader):\r\n",
        "    if torch.cuda.is_available():\r\n",
        "      images = images.cuda().detach()\r\n",
        "      boxes = [ann.cuda().detach() for ann in boxes]\r\n",
        "      masks = [mask.cuda().detach() for mask in masks]\r\n",
        "    loss_c, loss_b, loss_m, loss_s = net(images, boxes, masks)\r\n",
        "    loss_total = loss_c + loss_b + loss_m + loss_s\r\n",
        "    optimizer.zero_grad()\r\n",
        "    loss_total.backward()\r\n",
        "    optimizer.step()\r\n",
        "    this_time=time.time()\r\n",
        "    if i%30==0:\r\n",
        "      print(\"Time taken Batch no \",i,\" is \",this_time-time_last,\"Loss is \",loss_total.item())\r\n",
        "    time_last=time.time()\r\n",
        "\r\n",
        "  print(\"Epoch no \",step_no,\" is completed\")\r\n",
        "  save_latest(net,cfg.cfg_name,step_no)\r\n",
        "  step_no+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVfx9ikgeUK1"
      },
      "source": [
        "##################### Detection Begins Here ####################################\r\n",
        "from output_utils import *\r\n",
        "net=YOLACT().to(device)\r\n",
        "path=\"drive/My Drive/YOLACT/weights/latest_resnet_pascal_23.pth\"\r\n",
        "net.load_weights(path)\r\n",
        "net.eval()\r\n",
        "cfg.mode='detect'\r\n",
        "with torch.no_grad():\r\n",
        "\r\n",
        "  if cfg.test_image_path is not None:\r\n",
        "    dataset= PascalLoader(mode='detect')\r\n",
        "    data_loader=data.DataLoader(dataset,1,shuffle=False,pin_memory=True,collate_fn=detect_collate)\r\n",
        "    for i, (img, img_origin, img_name) in enumerate(data_loader):\r\n",
        "      \r\n",
        "      if torch.cuda.is_available():\r\n",
        "        img=img.cuda()\r\n",
        "        img_h,img_w=img_origin.shape[0:2]\r\n",
        "        class_p,box_p,coef_p,proto_p,anchors=net(img)\r\n",
        "\r\n",
        "        ids_p, class_p, box_p, coef_p, proto_p = nms(class_p, box_p, coef_p, proto_p, anchors, cfg)\r\n",
        "        ids_p, class_p, boxes_p, masks_p = after_nms(ids_p, class_p, box_p, coef_p,\r\n",
        "                                                             proto_p, img_h, img_w, cfg, img_name=img_name)\r\n",
        "        img_numpy = draw_img(ids_p, class_p, boxes_p, masks_p, img_origin, cfg, img_name=img_name)\r\n",
        "        cv2_imshow(img_numpy)\r\n",
        "        print(img_name)\r\n",
        "        if cfg.real_time:\r\n",
        "          cv2.imshow('Detection', frame_numpy)\r\n",
        "          cv2.waitKey(1)\r\n",
        "        else:\r\n",
        "          cv2.imwrite('drive/My Drive/YOLACT/results/'+img_name, img_numpy)\r\n",
        "  \r\n",
        "  elif cfg.video is not None:\r\n",
        "    vid = cv2.VideoCapture(cfg.video)\r\n",
        "    target_fps = round(vid.get(cv2.CAP_PROP_FPS))\r\n",
        "    frame_width = round(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\r\n",
        "    frame_height = round(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\r\n",
        "    num_frames = round(vid.get(cv2.CAP_PROP_FRAME_COUNT))\r\n",
        "\r\n",
        "    name = cfg.video.split('/')[-1]\r\n",
        "    video_writer = cv2.VideoWriter(f'drive/MyDrive/YOLACT/results/{name}', cv2.VideoWriter_fourcc(*\"mp4v\"), target_fps,\r\n",
        "                                       (frame_width, frame_height))\r\n",
        "    t_fps = 0\r\n",
        "    for i in range(num_frames):\r\n",
        "      frame_origin = vid.read()[1]\r\n",
        "      img_h, img_w = frame_origin.shape[0:2]\r\n",
        "      frame_trans = val_aug(frame_origin, cfg.img_size)\r\n",
        "      frame_tensor = torch.tensor(frame_trans).float()\r\n",
        "      if torch.cuda.is_available():\r\n",
        "        frame_tensor = frame_tensor.cuda()\r\n",
        "      class_p, box_p, coef_p, proto_p, anchors = net(frame_tensor.unsqueeze(0))\r\n",
        "      ids_p, class_p, box_p, coef_p, proto_p = nms(class_p, box_p, coef_p, proto_p, anchors, cfg)\r\n",
        "      ids_p, class_p, boxes_p, masks_p = after_nms(ids_p, class_p, box_p, coef_p,\r\n",
        "                                                             proto_p, img_h, img_w, cfg)\r\n",
        "      frame_numpy = draw_img(ids_p, class_p, boxes_p, masks_p, frame_origin, cfg, fps=t_fps)\r\n",
        "      video_writer.write(frame_numpy)\r\n",
        "    vid.release()\r\n",
        "    video_writer.release()\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yZVfbIcA5fF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}